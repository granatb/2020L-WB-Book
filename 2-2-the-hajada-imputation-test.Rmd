## The Hajada Imputation Test

*Authors: Jakub Kosterna, Dawid Przybyli≈Ñski, Hanna Zdulska (Warsaw University of Technology)*

### Abstract

There are many different ways of dealing with missing values. Depending on their quantity and properties, various methods turn out to be the best. The fact is, there is no one universal best method of imputation, and different problems require various solutions. Using one and the same solution for each dataset is certainly not the best option, but sometimes it is convenient or even necessary to focus on one particular method without much insight into its effect - for example, when time limits us or when, due to the complexity of the algorithm or class of problems, we are forced to choose only one method.

In this chapter which we decided to name *The Hajada Imputation Test* (taken from the first two letters of creators' names) we will take a look at several popular imputation methods and try to compare them with each other, including ranking of algorithms taken into account and finally choosing the best one. Obviously its result will not be the final verdict declaring which imputation algorithm is clearly better or worse, but considering the comparison of their quality on different datasets, we will be able to assess their collective performance and effectiveness against the backdrop of the whole. For this purpose, we will test the modified collections for getting rid of missing values on several machine learning classification algorithms, and then considering the selected quality measures after comparing the results, we will get the desired view. Due to the need to obtain a clear and interpretable result, we have limited ourselves to the problem of binary classification.

### Introduction and Motivation

In statistics, imputation is the process of replacing missing data with substituted values. Over the years, humanity has created many different methods accomplishing that, including the simple and instinctive ones but also more advanced and hard to be easily explained. Choosing the best-suited imputation method for the dataset with missing values is still the daily dilemma of every data scientist and there isn't one universally recognized best technique. Some scientists believe the crux lies in the most advanced and sophisticated algorithms, others trust the simplest of all possible. In a way, both parts are right - the first naturally win in speed of execution, but the second ones do not have advanced operations implemented for nothing and this usually results in data that more accurately reflects reality or what might be expected from it. How do the individual popular ways fall on the famous collections from OpenML100^1^? Many wondered, we decided to find the answer to this question. Many were wondering, but few answered this question - some interesting conclusions have been developed here.

#### Imputation functions

Six different methods have been taken into consideration. Their selection is well thought out - they are all widely known in the world of Data Science, and at the same time they differ significantly in approach, implementation, concept and results. Comparing them not only results in a fair view of the strictly indicated implementations, but also what result subsequent approaches result in.

Three simple imputation methods and three more advanced algorithms offered by popular packages were taken to consideration and their comparison. The clash of implementations with such a degree of diversity of complexity gives a clear message whether it pays off to reach for an advanced algorithm from the package prepared by professionals for many hours, conducted on many tests, or rather stay with minimalism and write easy code solving the problem in a trivial way.

Methods compared in subsequent stages are:

1. *mode / median replace* - basic process which puts in a place of missing data median cells from their columns for numerical values and dominants for categorical ones.
2. *remove rows* - trivial solution of removing rows containing any missing data.
3. *'bogo' random replace* - simple algorithm replacing NA values with random numbers or factors from their features.
4. *mice* ^2^ - advanced method creating multiple imputations for multivariate missing data, based on *Fully Conditional Specification* ^3^, where each incomplete variable is imputed by a separate model. Standard imputation with *maxit* parameter of value 0 and default 5 number of imputed datasets.
5. *VIM* ^4^ - standard k-nearest neighbors algorithm taken from the library.
6. *missForest* ^5^ - imputation based on random forest offered by the package.

#### Datasets

The fourteen data frames were taken from OpenML100 collection ^1^ and were corrected specifically for this research. Both small and simple frames were chosen as well as more complex and containing a lot of information, difficult to explain or present. They can be found under the following identifiers with the following names: 1590 - *adult*, 188 - *eucalyptus*, 23381 - *dresses-sales*, 29 - *credit-approval*, 38 - *sick*, 40536 - *SpeedDating*, 41278 - *okcupid-stem*, 56 - *vote*, 6332 - *cylinder-bands*, 1018 - *ipums_la_99-small*, 27 - *colic*, 4 - *labor*, 55 - *hepatitis* and 944 - *echoMonths*.

These above have been placed in individual directories identified by id in the prepared directory. The six imputations mentioned earlier were conducted on all fourteen of them, but only on six imputations were successful in their entirety.

#### Binary classification algorithms

In order to compare the quality of the imputed data and see how this supplementation of deficiencies works in practice, five binary classification algorithms have been selected. Their choice was made after careful analysis and extensive discussion, in order to find models that are both widely known and used, but also apply to different approaches and give reasonably distinguishable results using different techniques. The final choice fell on:

* *classification tree from rpart^6^* - classic algorithm which uses a decision tree to go from observations about an item to conclusions about the item's target value,
* *k-Nearest Neighbors^7^ from class^8^ package* - standard knn^7^ model attributing a given observation to a target corresponding to its closest observations in space,
* *naive Bayes from e1071^9^* - well-known classification computing the conditional a-posterior probabilities of a categorical class variable given independent predictor variables using the Bayes rule.
* *random forest from ranger^10^* - algorithm consisting of many decisions trees uses bagging and feature randomness when building each individual tree trying to create an uncorrelated forest of trees whose prediction by committee is more accurate than that of any individual tree.
* *Support Vector Machine from e1071^9^* -  a discriminative classifier formally defined by a separating hyperplane.

### Methology

#### Reading datasets

In order to read all datasets another function was used, which returned a dataframe containing all the important informations about the test matrices. The generated frames of imputation results and times were saved using parallel processing^11^.

#### Metrics functions

In order to test the same test-train splits we used random seed 1357 for all datasets. For each machine learning model after every imputation was created confusion matrix and the values of four basic metrics:

* *accuracy* - $\frac{TP+TN}{TP+FP+FN+TN}$
* *precision* - $\frac{TP}{TP+FP}$
* *recall* - $\frac{TP}{TP+FN}$
* *f1* - $2*\frac{Recall * Precision}{Recall + Precision}$

For the final conclusion, ranking concerned only the last one. Additionally, Matthews correlation coefficient^12^ measures were also computed.

${\displaystyle {\text{MCC}}={\frac {{\mathit {TP}}\times {\mathit {TN}}-{\mathit {FP}}\times {\mathit {FN}}}{\sqrt {({\mathit {TP}}+{\mathit {FP}})({\mathit {TP}}+{\mathit {FN}})({\mathit {TN}}+{\mathit {FP}})({\mathit {TN}}+{\mathit {FN}})}}}}$

These two give great information about the quality of imputation and a good comparison. It resulted in simple and clear information about which observations are well classified, which ones are wrong and how they should be. The main advantage of them is accounting for target variable's inbalance. Considering sets' different properties, even when it comes to balancing, comparing those measures, the fact that they show the pros and cons of given result will additionally be worthwhile to compare them.

### Results

#### Comparing imputation times

A dataframe containing information about imputation methods was built.

![](2-2-files/imputationTimesScatter.png)

Judging by the logarithmic scale, there is no surprise that removing rows and mode/median replenishment are definitely the fastest methods, with removing rows being several times faster. What would you expect, also "random values" imputation was quite fast, but slower than the two mentioned. Looking at more advanced ones, definitely *VIM* turned up to be the fastest - times usually being approximately 5-10 times shorter than *missForest* and *mice*. These last two are quite slow, with the former appearing to be slightly faster.

For a better view a boxplot was also created:

![](2-2-files/imputationTimesBoxplot.png)

Taking into account all the imputations which have been implemented, it is clearly seen that removing rows as well as replacing with mode or median really stand out. However, we cannot fully compare the other four - due to the fact that missForest failed on three datasets, and mice - up to seven, mostly for datasets greater in size.

Considering such a large spread of data size, there is a very interesting difference between median and mean for VIM's  time - the first is almost seven minutes, the second - barely a second and a half. In general, however, it is certainly much longer than methods for removing incomplete rows and filling with mode/median.

Also time was compared for those datasets for which all imputations were successful - these are ids 27, 38, 55, 56, 944, 188  (dataset with id 4 softened on removing rows containing any missing items, because each of its poems had some missing items). Taking into account the mean time, definitely *missForest* was the slowest, but also its standard deviation seemed to be incomparably large - this is probably due to the fact that for smaller sets it is doing well, but due to its complexity, its slowdown can be seen for very sizeable datasets. *VIM* turned out to be better for quick calculations than *missForest* and *mice*, and considering the median, *mice* is comparable to *missForest* - so for small datasets there is not much difference between them, and a lot of time was definitely needed to devote to these larger data frames.

#### Best measures

In order to compare *F1* and *MCC* measures, mean results on all sets for all imputations were calculated. Then, for each machine learning algorithm a ranking was created - so that the imputation with best results obtained the first rank, second best - 2nd, etc. The rank's distributions for every imputation came out as shown by the following boxplot:

![](2-2-files/rankingGood.png)

It might seem that the results of the methods are very close to each other, but the above visualization takes into account rankings only for successful imputations - to get the correct comparison there is a need to make a ranking with failing methods receiving the worst ranks instead of being omitted. The outcome was presented below:

![](2-2-files/rankingAll.png)

Considering the imputation times and results, it can be said that the knn^7^ algorithm from the VIM package proved to be "universally" the best. Out of the advanced methods it falls out the fastest, and its results are also the best. However, it is more difficult to choose second and subsequent places - looking at the results, the simple methods are very quick, but they differ from the more advanced ones. Looking at the advanced ones, with the exception of VIM, MissForest stands out. However, his problem is time. So after VIM, it seems second best to use mean / median replenishment for a small amount of time and missForest for a large one. However, although mode / medain usually obtained worse ranks, there was no extensive difference in results between the methods - which is not visible in the charts. "removeRows" is characterized by large fluctuations, and therefore we advise against using it.

#### Summary and conclusions

The experiment can be considered successful when it comes to the datasets and tools made available to us - calculated measures, visualizations made, as well as conclusions led us, among others, to the intuition that probably *VIM* is really a resplendent package, supplementing the missing values with medians from columns is not is such an unwise idea as it may seem and *missForest* is also noteworthy - if one has time for running it and the dataset isn't too sizeable. Unfortunately, it is impossible to draw an objective conclusion based on such a small number of tested data - with as few as 14 sets, problems with imputations on some of them or too small dataframes not giving a satisfactory answer. Nevertheless, the code is certainly very valuable and for more datasets it could confirm one or another belief.

#### References

*^1^OpenML100: a predecessor of the OpenML-CC18, a suite of 100 classification datasets from OpenML which were carefully selected to be usable by many algorithms and also represent datasets commonly used in machine learning research*

*^2^mice: Multivariate Imputation by Chained Equations, package created by Stef van Buuren, Karin Groothuis-Oudshoorn, Gerko Vink, Rianne Schouten, Alexander Robitzsch, Lisa Doove, Shahab Jolani, Margarita Moreno-Betancur, Ian White, Philipp Gaffert, Florian Meinfelder, Bernie Gray*

*^3^Fully conditional specification: Imputes multivariate missing data on a variable-by-variable basis (Van Buuren et al. 2006; Van Buuren 2007a).*

*^4^VIM: Visualization and Imputation of Missing Values, package created by Matthias Templ, Alexander Kowarik, Andreas Alfons, Bernd Prantner*

*^5^missForest: Nonparametric Missing Value Imputation using Random Forest Daniel J. Stekhoven*

*^6^rpart: Recursive Partitioning and Regression Trees, package by Terry Therneau, Beth Atkinson, Brian Ripley*

*^7^k-nearest neighbors algorithm: a non-parametric method with the input consisting of the k closest training examples in the feature space.*

*^8^class: Various functions for classification, including k-nearest neighbour, Learning Vector Quantization and Self-Organizing Maps, package created by Brian Ripley, William Venables*

*^9^e1071: Misc Functions of the Department of Statistics, Probability Theory Group (Formerly: E1071), TU Wien, package created by David Meyer*

*^10^ranger: A Fast Implementation of Random Forests, package created by Marvin N. Wright, Stefan Wager, Philipp Probst*

*^11^parallel: Support for parallel computation, including by forking (taken from package multicore), by sockets (taken from package snow) and random-number generation*

*^12^Matthews correlation coefficient: a measure of the quality of binary (two-class) classifications, introduced by biochemist Brian W. Matthews in 1975*

*^11^Matthews correlation coefficient: a measure of the quality of binary (two-class) classifications, introduced by biochemist Brian W. Matthews in 1975*